# OOM Cases in Spark
## [Apache Spark User List](http://apache-spark-user-list.1001560.n3.nabble.com/)

Found 191 matching posts for `oom` in Apache Spark User [List](http://apache-spark-user-list.1001560.n3.nabble.com/template/NamlServlet.jtp?macro=search_page&node=1&query=oom).

at [here](http://apache-spark-user-list.1001560.n3.nabble.com/template/NamlServlet.jtp?macro=search_page&node=1&query=%22out+of+memory%22&days=0&i=96)
at [here](http://apache-spark-user-list.1001560.n3.nabble.com/template/NamlServlet.jtp?macro=search_page&node=1&query=%22out+of+memory%22&days=0&i=36)
[JIRA](https://issues.apache.org/jira/browse/SPARK-671?jql=project%20%3D%20SPARK%20AND%20text%20~%20%22out%20of%20memory%22)
Labels:

- Driver (Dr): error occurs in driver
- Executor (Ex): error occurs in executor
- Solved (S): this problem has been solved
- Unsolved (U):
	- R: we can reproduce the problem for further study  
	- D: just description which cannot be reproduced
- V: valuable 

## Spark 
1. [trouble with "join" on large RDDs](http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-quot-join-quot-on-large-RDDs-td3864.html#a4039) (Ex, R)

	The join consistently crashes at the beginning of the reduce phase. 
	Note that when joining the 10G RDD to itself there is no problem. 
2. [Driver OOM while using reduceByKey](http://apache-spark-user-list.1001560.n3.nabble.com/Driver-OOM-while-using-reduceByKey-td6513.html) (Dr, S)

	too many tasks generated by reduceByKey() => control partitions => limit the number of reduce task
3. [OOM - Help Optimizing Local Job](http://apache-spark-user-list.1001560.n3.nabble.com/OOM-Help-Optimizing-Local-Job-td643.html) (Ex, R, V)

	HashMap used in user code.
4. [spark master OOME from maxMbInFlight buffers](http://apache-spark-user-list.1001560.n3.nabble.com/spark-master-OOME-from-maxMbInFlight-buffers-td1441.html) (Dr, D)

	The heap dump shows 70 byte[]s, owned by various Akka threads, all 48mb 
each (3.3gb total), which I assume is from the maxMbInFlight value. 
5. [GroupByKey results in OOM - Any other alternative](http://apache-spark-user-list.1001560.n3.nabble.com/GroupByKey-results-in-OOM-Any-other-alternative-td7625.html) (Ex, S, V)

	groupByKey().map( x => (x_1, x.\_2.distinct)).map(x => (x_1, x.\_2.distinct.count))
	
	Solution: You can do a little better than grouping *all* values and *then*  finding distinct values by using foldByKey(), putting values into a Set. Or countApproxDistinctByKey().
	
6. [Kyro serialization slow and runs OOM](http://apache-spark-user-list.1001560.n3.nabble.com/Kyro-serialization-slow-and-runs-OOM-td1073.html) (Ex, R)
	
	 I load my dataset, transform it with some one to one transformations, and try to cache the eventual RDD - it runs really slow and then runs out of memory. When I remove Kyro serializer and default back to java serialization it works just fine.
7. [OOM when calling cache on RDD with big data](http://apache-spark-user-list.1001560.n3.nabble.com/OOM-when-calling-cache-on-RDD-with-big-data-td1894.html) (Ex, R)

	I have a very simple job that simply caches the hadoopRDD by calling cache/persist on it. I tried MEMORY_ONLY, MEMORY_DISK and DISK_ONLY for caching strategy, I always get OOM on executors.
8. [how to set spark.executor.memory and heap size](http://apache-spark-user-list.1001560.n3.nabble.com/how-to-set-spark-executor-memory-and-heap-size-td4719.html#a7469)

	val logData = sc.parallelize(Array(1,2,3,4)).cache()
	
	Memory space is small.
9. [distinct on huge dataset](http://apache-spark-user-list.1001560.n3.nabble.com/distinct-on-huge-dataset-td3025.html#a3037) 
	
	I have a huge 2.5TB file. When i run: 
	
	val t = sc.textFile("/user/hdfs/dump.csv")   
	t.distinct.count 
10. [Does foreach operation increase rdd lineage?](http://apache-spark-user-list.1001560.n3.nabble.com/Does-foreach-operation-increase-rdd-lineage-td879.html#a881)

	Do you mean "Gibbs sampling" ? Actually, foreach is an action, it will collect all data from workers to driver. You will get OOM complained by JVM.
	
	Each update of the state of your markov chain should be a new RDD. I've found that I can do this for 100 or 200 iterations and then I'll get a stack overflow (presumably because the lineage is growing too large.) To get around this you either need to occasionally collect the RDD or write it to disk. Or just checkpoint() it.
	
11. [Fwd: Spark - ready for prime time?](http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-Spark-ready-for-prime-time-td4064.html#a4183)

	The biggest issue I've come across is that the cluster is somewhat unstable when under memory pressure.  Meaning that if you attempt to persist an RDD that's too big for memory, even with MEMORY_AND_DISK, you'll often still get OOMs.  I had to carefully modify some of the space tuning parameters and GC settings to get some jobs to even finish.

	The other issue I've observed is if you group on a key that is `highly skewed`, with a few massively-common keys and a long tail of rare keys, the one massive key can be too big for a single machine and again cause OOMs.
	
	I agree with Andrew....Every time I `underestimate` the RAM requirement....my hand calculations are always ways less than what JVM actually allocates...
	
12. [Stream RDD to local disk](http://apache-spark-user-list.1001560.n3.nabble.com/Stream-RDD-to-local-disk-td1045.html)

	But if I .collect() on the driver and then save to disk using normal Scala disk IO utilities, I'll certainly OOM the driver.
13. [KafkaInputDStream mapping of partitions to tasks](http://apache-spark-user-list.1001560.n3.nabble.com/KafkaInputDStream-mapping-of-partitions-to-tasks-td3360.html#a3382)

	But with kafka, we can consume more rdds later, after we finish previous rdds. 
That way it would be much much simpler to not get OOMâ€™ed when starting from beginning, 
because we can consume many data from kafka during batch duration and then get oom. 
14. [Questions about productionizing spark](http://apache-spark-user-list.1001560.n3.nabble.com/Questions-about-productionizing-spark-td4825.html)

	All went well except the last task of join. The job always stucked there, and lead eventually to OOM.  Even if we give sufficient memory and the job finally pass, the last task of join took significantly more time than other tasks (say several minutes vs 200ms). 
15. [Not getting it](http://apache-spark-user-list.1001560.n3.nabble.com/Not-getting-it-td3316.html#a3437)

	For the CSV file I used 1024 partitions [textFile(path, 1024)] which cut the partition size down to 8MB (based on standard HDFS 64MB splits).  For the key file I also adjusted partitions to use about 8MB.  This was still blowing up with GC Overlimit and Heap OOM with join.  I then set SPARK_MEM (which is hard to tease out of the documentation) to 4g and the join completed.
16. [newbie : java.lang.OutOfMemoryError: Java heap space](http://apache-spark-user-list.1001560.n3.nabble.com/newbie-java-lang-OutOfMemoryError-Java-heap-space-td365.html)

	mapper join with code.
	
	From the stack trace, it looks like the driver program is dying trying to serialize data out to the workers. 
	
17. [Using Spark on Data size larger than Memory size](http://apache-spark-user-list.1001560.n3.nabble.com/Using-Spark-on-Data-size-larger-than-Memory-size-td6589.html#a6642)

	In general, one problem with Spark today is that you can OOM under certain configurations, and it's possible you'll need to change from the default configuration if you're using doing very memory-intensive jobs. However, there are very few cases where Spark would simply fail as a matter of course -- for instance, you can always increase the number of partitions to decrease the size of any given one. or repartition data to eliminate skew.
	
18. [OutofMemory: Failed on spark/examples/bagel/WikipediaPageRank.scala](http://apache-spark-user-list.1001560.n3.nabble.com/OutofMemory-Failed-on-spark-examples-bagel-WikipediaPageRank-scala-td6040.html)

	I am running a 30GB Wikipedia dataset on a 7-server cluster. Using WikipediaPageRank underexample/Bagel.

	The problem is that the job will fail after several stages because of OutofMemory Error. The reason might be that the default executor's memory size is 512M.

19. [How to efficiently join this two complicated rdds](http://apache-spark-user-list.1001560.n3.nabble.com/How-to-efficiently-join-this-two-complicated-rdds-td1665.html#a1749)

	we have implement this way, we use pyspark, and standalone mode. We collect the new RDD2 in each iteration. The java heap memory costed by the driver program increases Gradually. And finally Collapse with OutOfMemory Error.
	
20. [OutOfMemoryError with basic kmeans](http://apache-spark-user-list.1001560.n3.nabble.com/OutOfMemoryError-with-basic-kmeans-td1651.html#a1653)

	I'm trying to run a basic version of kmeans (not the mllib version), on 250gb of data on 8 machines (each with 8 cores, and 60gb of ram).  I've tried many configurations, but keep getting an OutOfMemory error (at the bottom). 

21. [Serializer or Out-of-Memory issues?](http://apache-spark-user-list.1001560.n3.nabble.com/Serializer-or-Out-of-Memory-issues-td8533.html)

	ERROR TaskSchedulerImpl: Lost executor 0 on localhost: OutOfMemoryError
	
22. [Long running time for GraphX pagerank in dataset com-Friendster](http://apache-spark-user-list.1001560.n3.nabble.com/Long-running-time-for-GraphX-pagerank-in-dataset-com-Friendster-td4511.html#a4533)

	I was running some pagerank tests of GraphX in my 8 nodes cluster. I allocated each worker 32G memory and 8 CPU cores. The LiveJournal dataset used 370s, which in my mind is reasonable. But when I tried the com-Friendster data triggers out of memory.

23. [Partitioning - optimization or requirement?](http://apache-spark-user-list.1001560.n3.nabble.com/Partitioning-optimization-or-requirement-td3359.html)

	I'm often running out of memory when doing unbalanced joins (ie. cardinality of some joined elements much larger than others).  Raising the memory ceiling fixes the problem, but that's *a slippery slope*.
	
24. [trouble with broadcast variables on pyspark](http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-broadcast-variables-on-pyspark-td1301.html#a1308)

	The driver JVM is hitting OutOfMemoryErrors, but the python process is taking even more memory. 

25. [Exception in thread "DAGScheduler" java.lang.OutOfMemoryError: GC overhead limit exceeded](http://apache-spark-user-list.1001560.n3.nabble.com/Exception-in-thread-quot-DAGScheduler-quot-java-lang-OutOfMemoryError-GC-overhead-limit-exceeded-td833.html)
	
	I run a job that plans 3105 tasks. 3104 tasks out of 3105 tasks run OK, then the tasks start failing and after 36K failed tasks, I get following. Is the master running out of memory ?
26. [Spark streaming on load run - How to increase single node capacity?](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-streaming-on-load-run-How-to-increase-single-node-capacity-td6953.html)
	Increased the number of working threads from local[2] to local[4] and local[8] and started getting: 
	 - outofmemoryerror: could not create native thread. 
	- RDD elements were being processed as empty values - as if the garbage collector or something inside Spark was cleaning them before time. 

27. [Memory allocation in the driver](http://apache-spark-user-list.1001560.n3.nabble.com/Memory-allocation-in-the-driver-td8406.html)

	If you read this and have heap allocation bothering you on calling "first". To get rid of the heap allocation error, increase the driver's memory or shrink size of first partition.
	
28. [Akka error with largish job (works fine for smaller versions)](http://apache-spark-user-list.1001560.n3.nabble.com/Akka-error-with-largish-job-works-fine-for-smaller-versions-td3097.html#a3209)
	
	all the workers ran out of memory.
	
	All I'm doing is data.map(r => (getKey(r),r)).sortByKey().map(_._2).coalesce(n).saveAsTextFile(), where n is the original number of files in the dataset.
	
	I took a look at a workers memory before it ran out using jmap and jhat; they indicated file handles as the biggest memory user (which I guess makes sense for a sort) - but the total was nowhere close to 200g
29. [Spark limitations question](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-limitations-question-td3296.html)

	I have two tests: 
	1. join Base to itself, sum the "nums" and write out to HDFS 
	2. same as 1 except join Base to Skewed 
30. [Memory vs. disk - how to spill data into disk](http://apache-spark-user-list.1001560.n3.nabble.com/Memory-vs-disk-how-to-spill-data-into-disk-td1383.html)

	
31. [Spark stalling during shuffle (maybe a memory issue)](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-stalling-during-shuffle-maybe-a-memory-issue-td6067.html)

	At this point Spark-related activity on the hadoop cluster completely halts .. there's no network activity, disk IO or CPU activity, and individual tasks are not completing and the job just sits in this state.  At this point we just kill the job & a re-start of the Spark server service is required. 
	
	So we upped the spark.akka.frameSize value to 128 MB and still observed the same behavior.  It's happening not necessarily when data is being sent back to the driver, but when there is an inter-cluster shuffle, for example during a groupByKey.
	
	If the distribution of the keys in your groupByKey is skewed (some keys appear way more often than others) you should consider modifying your job to use reduceByKey instead wherever possible.
32. [pyspark join crash](http://apache-spark-user-list.1001560.n3.nabble.com/pyspark-join-crash-td6938.html)

	In PySpark, the data processed by each reduce task needs to fit in memory within the Python process, so you should use more tasks to process this dataset. Data is spilled to disk across tasks. 
	
	I think the problem is that once unpacked in Python, the objects take considerably more space, as they are stored as Python objects in a Python dictionary. 
	
	Iâ€™m not sure whether thereâ€™s a great way to inspect a Python processâ€™s memory, but looking at what consumes memory in a reducer process would be useful. 
	
33. [FileNotFoundException when using persist(DISK_ONLY)](http://apache-spark-user-list.1001560.n3.nabble.com/FileNotFoundException-when-using-persist-DISK-ONLY-td7291.html)

	I'm thinking that the FileNotFoundExceptions are due to tasks being cancelled/restarted and the root cause is the OutOfMemoryError.
	
	Otherwise, I figure next steps would be to enable more debugging levels in the spark code to see what much memory the code is trying to allocate. At this point, I'm wondering if the block could be in the GB range.

34. [spark streaming rate limiting from kafka](http://apache-spark-user-list.1001560.n3.nabble.com/spark-streaming-rate-limiting-from-kafka-td8590.html#a8592)

	In my use case, if I need to stop spark streaming for a while, data would accumulate a lot on kafka topic-partitions. After I restart spark streaming job, the worker's heap will go out of memory on the fetch of the 1st batch.

35. [advice on maintaining a production spark cluster?](http://apache-spark-user-list.1001560.n3.nabble.com/advice-on-maintaining-a-production-spark-cluster-td5848.html#a6124)

	We've had occasional problems with running out of memory on the driver side (esp. with large broadcast variables) so that may be related.  

36. [Spark Processing Large Data Stuck](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-Processing-Large-Data-Stuck-td8075.html#a8077)
	
	I run the pagerank example processing a large data set, 5GB in size, using 48 machines. The root cause is the out of memory error - verified this by monitoring the memory. 
	
37. [Problems with broadcast large datastructure](http://apache-spark-user-list.1001560.n3.nabble.com/Problems-with-broadcast-large-datastructure-td331.html#a491)
	
	If your object size > 10MB you may need to change spark.akka.frameSize.

	400MB isn't really that big. Broadcast is expected to work with several GB of data and in even larger clusters (100s of machines).
	
	if you are using the default HttpBroadcast, then akka isn't used to move the broadcasted data. But block manager can run out of memory if you repetitively broadcast large objects. Another scenario is that the master isn't receiving any heartbeats from the blockmanager because the control messages are getting dropped due to bulk data movement. Can you provide a bit more details on your network setup?
	

	
##Spark (D)
1. [com.google.protobuf out of memory](http://apache-spark-user-list.1001560.n3.nabble.com/com-google-protobuf-out-of-memory-td6357.html#a6373) (D)
	
	I am getting a OutOfMemoryError in class ByteString.java from package com.google.protobuf when processing very large data using spark 0.9. 

2. [Kafka streaming out of memory](http://apache-spark-user-list.1001560.n3.nabble.com/Kafka-streaming-out-of-memory-td2639.html) (D)

	 When I use the kafka streaming with spark, the rdds store in memory and never be realesed, so OOM will happen after time. 	

3. [Maximum memory limits](http://apache-spark-user-list.1001560.n3.nabble.com/Maximum-memory-limits-td2717.html) (D)

	I was thinking that jblas is going to call native malloc right? 
4. [Pyspark Memory Woes](http://apache-spark-user-list.1001560.n3.nabble.com/Pyspark-Memory-Woes-td2538.html#a4093) (D)

	We're seeing various OOM problems: sometimes python takes all available mem, sometimes we OOM with no heap space left, and occasionally OOM with GC overhead limit exceeded.  I realise it's hard to troubleshoot in the absence of code but any test case we have would be contrived. It seems we should be setting it to (SPARK_WORKER_MEMORY + pyspark memory) / # of concurrent applications, but is there any advice on how to balance memory between executors and pyspark, or does it depend too much on the workload? How do we know if we're getting the most bang for our buck, so to speak?
5. [Spark 0.9.1 java.lang.outOfMemoryError: Java Heap Space](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-0-9-1-java-lang-outOfMemoryError-Java-Heap-Space-td7861.html)

	I am trying to process a file that contains 4 log lines (not very long) and then write my parsed out case classes to a destination folder, and I get the following error:
	


## MLlib (E)
1.  [Running out of memory Naive Bayes](http://apache-spark-user-list.1001560.n3.nabble.com/Running-out-of-memory-Naive-Bayes-td4866.html#a4876) (E)
	
	Each example in the dataset is about 2 million features, only about 20-50 of which are non-zero, so the vectors are very sparse. I keep running out of memory though, even for about 1000 examples on 30gb RAM while the entire dataset is 4 million examples.
	
## Spark Streaming
1. [Out of memory when spark streaming](http://apache-spark-user-list.1001560.n3.nabble.com/help-me-Out-of-memory-when-spark-streaming-td5854.html)

	I send data to spark streaming through Zeromq at a speed of 600 records per second, but the spark streaming only handle 10 records per 5 seconds( set it in streaming program)
	
## Memory-related issues
1. [Spark Memory Bounds](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-Memory-Bounds-td6456.html#a6500) (VD)

	Spark's memory usage is roughly the following:

	(A) In-Memory RDD use + (B) In memory Shuffle use + (C) Transient memory used by all currently running tasks.

2. [Pyspark Memory Woes](http://apache-spark-user-list.1001560.n3.nabble.com/Pyspark-Memory-Woes-td2538.html)

	We're seeing various OOM problems: sometimes python takes all available mem, sometimes we OOM with no heap space left, and occasionally OOM with GC overhead limit exceeded.
3. [ExternalAppendOnlyMap: Spilling in-memory map](http://apache-spark-user-list.1001560.n3.nabble.com/ExternalAppendOnlyMap-Spilling-in-memory-map-td6186.html#a6221)

	 The ExternalAppendOnlyMap is used when a shuffle is causing too much data to be held in memory.  Rather than OOM'ing, Spark writes the data out to disk in a sorted order and reads it back from disk later on when it's needed.  That's the job of the ExternalAppendOnlyMap.
4. [Local Standalone Application and shuffle spills](http://apache-spark-user-list.1001560.n3.nabble.com/Local-Standalone-Application-and-shuffle-spills-td2634.html#a2680)

	"spark.shuffle.spill" refers to a different behavior -- if the "reduce" phase of your shuffle would otherwise cause Spark to OOM, it will instead write data to temporary files on disk. You probably don't want to disable this unless you'd prefer to tune Spark to make sure the reduce can stay in memory.
5. [Performance and serialization: use case](http://apache-spark-user-list.1001560.n3.nabble.com/Performance-and-serialization-use-case-td1513.html#a1615)
	- SERIALIZATION: 
		- Is it OK to manipulate RDDs of Record objects or should we stick with simple RDDs of strings, and do all the splitting and computation in each transformation ? 
		- How to make that efficient in terms of memory and speed? 
		- I've read the docs about the Tuning (and Kryo serialization) but I'd like to have more info on that... 
	- PERFORMANCE: 
		- Is it a good idea to perform all the filters first, and then the groupBy customer, or should we do the reverse? 
		- In the second situation, how can we filter on the values? I didn't see a filterValues method in the PairRDD API ? 
6. [computation slows down 10x because of cached RDDs](http://apache-spark-user-list.1001560.n3.nabble.com/computation-slows-down-10x-because-of-cached-RDDs-td2480.html)

	I think what happened is the following: all the nodes generated some garbage that put them very close to the threshold for a full GC in the first few runs of the program (when you cached the RDDs), but on the subsequent queries, only a few nodes are hitting full GC per query, so every query sees a slowdown but the problem persists for a while.

## Interesting questions

1. Is there a way to know which key,value pair is resulting in the OOM?
2. Is there a way to set parallelism in the map stage so that each worker will process one key at time?
3. I was wondering how Spark handles congestion when the upstream is generating dstreams faster than downstream workers can handle? It will eventually OOM.
4. Is there a reason that this value couldn't be dynamically adjusted in response to actual heap usage?
5. Is there a way to automatically re-spawn spark workers? We've situations where executor OOM causes worker process to be DEAD and it does not came back automatically. 
6. Why am I seeing OOMs to begin with? 
7.  How can I reduce the likelihood of seeing OOMs 
8.  Why does an OOM seem to break the executor so hopelessly?
9.  By default 66% of the executor memory is used for RDD caching, so if there's no explicit caching in the code (eg. rdd.cache(), rdd.persiste(StorageLevel.MEM_AND_DISK) etc), this ram is wasted?
10. Is it possible to get the memory usage of one single task in JVM with GC running in the background? 

	you could run 1-core slaves. That way they would only work on one task at a time.
	
	The way I understand it, Spark does not have a tight control on the memory. Your code running on the executor can easily use more than 40% of memory. Spark only limits the memory used for RDD caches and shuffles. If its RDD caches are full, taking up 60% of the heap, and your code takes up more than 40% (after GC), the executor will die with OOM.
	
	heap dump => thread analysis 
	
	Try [heapaudit](https://github.com/foursquare/heapaudit)

11. I want to know that in the case when the size of HBase Table grows larger than the size of RAM available in the cluster, will the application fail, or will there be an impact in performance?
12. What I can not understand is there any easy way to say when we have out of memory situation please spill data into disk.

## Solutions:
1. If the foldByKey solution doesn't work for you, my team uses RDD.persist(DISK_ONLY) to avoid OOM errors.
2. Is your RDD of Strings?  If so, you should make sure to use the Kryo serializer instead of the default Java one.  It stores strings as UTF8 rather than Java's default UTF16 representation, which can save you half the memory usage in the right situation.
3. If an individual partition becomes too large to fit in memory then the usual approach would be to repartition to more partitions, so each one is smaller. Hopefully then it would fit.
4. resource contention. -Xmx cannot be achieved.
5. 


## Knowledge
1. When you launch each job, you set SPARK_MEM in its environment to control executors' memory usage. So for example SPARK_MEM=10g java -jar MyCode.jar. SPARK_MEM will automatically be propagated to the slaves. Although you can also set SPARK_MEM in conf/spark-env.sh, it's recommended not to do this, because that would set the same value for every job; the ability to set it there is just left-over from previous versions of Spark and we eventually want to remove it. (I realize it's confusing though.) 

2. SPARK_WORKER_MEMORY is only there to tell each worker daemon the *total* amount of memory on the machine, so that it can know how many concurrent jobs it can safely run (based on each job's SPARK_MEM). In 99% of cases, you will not have to set SPARK_WORKER_MEMORY; the Worker process automatically detects the amount of memory available. 

3. [KafkaInputDStream mapping of partitions to tasks](http://apache-spark-user-list.1001560.n3.nabble.com/KafkaInputDStream-mapping-of-partitions-to-tasks-td3360.html#a3378)
4.  In general, one problem with Spark today is that you can OOM under certain configurations, and it's possible you'll need to change from the default configuration if you're using doing very memory-intensive jobs. However, there are very few cases where Spark would simply fail as a matter of course -- for instance, you can always increase the number of partitions to decrease the size of any given one. or repartition data to eliminate skew.
5.  groupByKey is an expensive transformation, and collecting all the data to driver side may simply cause OOM if the data canâ€™t fit in the driver node.
6.  If your job is using external sorting to avoid OOMing (which it will warn you about in the executor logs with messages like "Spilling in-memory map..."), then you may have arbitrarily many files open. This is very unlikely to happen if you've split your input into as many files as you said, though.
7.  The more I think about it the problem is not about /tmp, its more about the workers not having enough memory. Blocks of received data could be falling out of memory before it is getting processed. BTW, what is the storage level that you are using for your input stream? If you are using MEMORY_ONLY, then try MEMORY_AND_DISK. That is safer because it ensure that if received data falls out of memory it will be at least saved to disk.
8. "In all cases, we recommend allocating only at most 75% of the memory for Spark; leave the rest for the operating system and buffer cache."
9. log of JVM

		# Native memory allocation (malloc) failed to allocate 2097152 bytes for committing reserved memory.
		# Possible reasons:
		#   The system is out of physical RAM or swap space
		#   In 32 bit mode, the process size limit was hit
		# Possible solutions:
		#   Reduce memory load on the system
		#   Increase physical memory or swap space
		#   Check if swap backing store is full
		#   Use 64 bit Java on a 64 bit OS
		#   Decrease Java heap size (-Xmx/-Xms)
		#   Decrease number of Java threads
		#   Decrease Java thread stack sizes (-Xss)
		#   Set larger code cache with -XX:ReservedCodeCacheSize=
		# This output file may be truncated or incomplete.
		#
		#  Out of Memory Error (os_linux.cpp:2761), pid=31426, tid=139549745604352
		#
		# JRE version: OpenJDK Runtime Environment (7.0_51-b02) (build 1.7.0_51-mockbuild_2014_01_15_01_39-b00)
		# Java VM: OpenJDK 64-Bit Server VM (24.45-b08 mixed mode linux-amd64 ) 
10. Spark automatically removes old RDDs from the cache when you make new ones. Unpersist forces it to remove them right away. In both cases though, note that Java doesnâ€™t garbage-collect the objects released until later.

11. In my experiment, I have 20 machine, each machine own 2 executor, and I used the default parallelize, which is 8, so there  320  tasks in one stage in total.

	Then the workers will send 320*(400M/8)=16G data back to the driver, this seem very big. but I get from log that after serialize, the data size send back to driver is just 446 byte in each task. 
	
	broadcast is supposed to send data from the driver to the executors and not the other direction. 
	
	Size calculation is correct, but broadcast happens from the driver to the workers. 

	btw, your code is broadcasting 400MB 30 times, which are not being evicted from the cache fast enough, which, I think, is causing blockManagers to run out of memory.

## Ideas
1. display and control the memory usage of code.
2. off-heap approach
3. weak references
4. let the error occurs in single node, then can help debug
5. test on small dataset
6. monitor the memory usage and estimate the data size and then decide to how to store the data
7. add counters
8. detection of memory-bloat

## Problem
1. data becomes large
2. configuration
3. 
