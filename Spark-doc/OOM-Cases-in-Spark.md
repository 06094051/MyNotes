# OOM Cases in Spark
## [Apache Spark User List](http://apache-spark-user-list.1001560.n3.nabble.com/)

Found 191 matching posts for `oom` in Apache Spark User [List](http://apache-spark-user-list.1001560.n3.nabble.com/template/NamlServlet.jtp?macro=search_page&node=1&query=oom).

Labels:

- Driver (Dr): error occurs in driver
- Executor (Ex): error occurs in executor
- Solved (S): this problem has been solved
- Unsolved (U):
	- R: we can reproduce the problem for further study  
	- D: just description which cannot be reproduced
- V: valuable 

## Spark 
1. [trouble with "join" on large RDDs](http://apache-spark-user-list.1001560.n3.nabble.com/trouble-with-quot-join-quot-on-large-RDDs-td3864.html#a4039) (Ex, R)

	The join consistently crashes at the beginning of the reduce phase. 
	Note that when joining the 10G RDD to itself there is no problem. 
2. [Driver OOM while using reduceByKey](http://apache-spark-user-list.1001560.n3.nabble.com/Driver-OOM-while-using-reduceByKey-td6513.html) (Dr, S)

	too many tasks generated by reduceByKey() => control partitions => limit the number of reduce task
3. [OOM - Help Optimizing Local Job](http://apache-spark-user-list.1001560.n3.nabble.com/OOM-Help-Optimizing-Local-Job-td643.html) (Ex, R, V)

	HashMap used in user code.
4. [spark master OOME from maxMbInFlight buffers](http://apache-spark-user-list.1001560.n3.nabble.com/spark-master-OOME-from-maxMbInFlight-buffers-td1441.html) (Dr, D)

	The heap dump shows 70 byte[]s, owned by various Akka threads, all 48mb 
each (3.3gb total), which I assume is from the maxMbInFlight value. 
5. [GroupByKey results in OOM - Any other alternative](http://apache-spark-user-list.1001560.n3.nabble.com/GroupByKey-results-in-OOM-Any-other-alternative-td7625.html) (Ex, S, V)

	groupByKey().map( x => (x_1, x.\_2.distinct)).map(x => (x_1, x.\_2.distinct.count))
	
	Solution: You can do a little better than grouping *all* values and *then*  finding distinct values by using foldByKey(), putting values into a Set. Or countApproxDistinctByKey().
	
6. [Kyro serialization slow and runs OOM](http://apache-spark-user-list.1001560.n3.nabble.com/Kyro-serialization-slow-and-runs-OOM-td1073.html) (Ex, R)
	
	 I load my dataset, transform it with some one to one transformations, and try to cache the eventual RDD - it runs really slow and then runs out of memory. When I remove Kyro serializer and default back to java serialization it works just fine.
7. [OOM when calling cache on RDD with big data](http://apache-spark-user-list.1001560.n3.nabble.com/OOM-when-calling-cache-on-RDD-with-big-data-td1894.html) (Ex, R)

	I have a very simple job that simply caches the hadoopRDD by calling cache/persist on it. I tried MEMORY_ONLY, MEMORY_DISK and DISK_ONLY for caching strategy, I always get OOM on executors.
8. [how to set spark.executor.memory and heap size](http://apache-spark-user-list.1001560.n3.nabble.com/how-to-set-spark-executor-memory-and-heap-size-td4719.html#a7469)

	val logData = sc.parallelize(Array(1,2,3,4)).cache()
	
	Memory space is small.
9. [distinct on huge dataset](http://apache-spark-user-list.1001560.n3.nabble.com/distinct-on-huge-dataset-td3025.html#a3037) 
	
	I have a huge 2.5TB file. When i run: 
	
	val t = sc.textFile("/user/hdfs/dump.csv")   
	t.distinct.count 
10. [Does foreach operation increase rdd lineage?](http://apache-spark-user-list.1001560.n3.nabble.com/Does-foreach-operation-increase-rdd-lineage-td879.html#a881)

	Do you mean "Gibbs sampling" ? Actually, foreach is an action, it will collect all data from workers to driver. You will get OOM complained by JVM.
	
	Each update of the state of your markov chain should be a new RDD. I've found that I can do this for 100 or 200 iterations and then I'll get a stack overflow (presumably because the lineage is growing too large.) To get around this you either need to occasionally collect the RDD or write it to disk. Or just checkpoint() it.
	
11. [Fwd: Spark - ready for prime time?](http://apache-spark-user-list.1001560.n3.nabble.com/Fwd-Spark-ready-for-prime-time-td4064.html#a4183)

	The biggest issue I've come across is that the cluster is somewhat unstable when under memory pressure.  Meaning that if you attempt to persist an RDD that's too big for memory, even with MEMORY_AND_DISK, you'll often still get OOMs.  I had to carefully modify some of the space tuning parameters and GC settings to get some jobs to even finish.

	The other issue I've observed is if you group on a key that is `highly skewed`, with a few massively-common keys and a long tail of rare keys, the one massive key can be too big for a single machine and again cause OOMs.
	
	I agree with Andrew....Every time I `underestimate` the RAM requirement....my hand calculations are always ways less than what JVM actually allocates...
	
12. [Stream RDD to local disk](http://apache-spark-user-list.1001560.n3.nabble.com/Stream-RDD-to-local-disk-td1045.html)

	But if I .collect() on the driver and then save to disk using normal Scala disk IO utilities, I'll certainly OOM the driver.
13. [KafkaInputDStream mapping of partitions to tasks](http://apache-spark-user-list.1001560.n3.nabble.com/KafkaInputDStream-mapping-of-partitions-to-tasks-td3360.html#a3382)

	But with kafka, we can consume more rdds later, after we finish previous rdds. 
That way it would be much much simpler to not get OOMâ€™ed when starting from beginning, 
because we can consume many data from kafka during batch duration and then get oom. 
14. [Questions about productionizing spark](http://apache-spark-user-list.1001560.n3.nabble.com/Questions-about-productionizing-spark-td4825.html)

	All went well except the last task of join. The job always stucked there, and lead eventually to OOM.  Even if we give sufficient memory and the job finally pass, the last task of join took significantly more time than other tasks (say several minutes vs 200ms). 
15. [Not getting it](http://apache-spark-user-list.1001560.n3.nabble.com/Not-getting-it-td3316.html#a3437)

	For the CSV file I used 1024 partitions [textFile(path, 1024)] which cut the partition size down to 8MB (based on standard HDFS 64MB splits).  For the key file I also adjusted partitions to use about 8MB.  This was still blowing up with GC Overlimit and Heap OOM with join.  I then set SPARK_MEM (which is hard to tease out of the documentation) to 4g and the join completed.
16. [newbie : java.lang.OutOfMemoryError: Java heap space](http://apache-spark-user-list.1001560.n3.nabble.com/newbie-java-lang-OutOfMemoryError-Java-heap-space-td365.html)

	mapper join with code.
	
	From the stack trace, it looks like the driver program is dying trying to serialize data out to the workers. 
	
17. [Using Spark on Data size larger than Memory size](http://apache-spark-user-list.1001560.n3.nabble.com/Using-Spark-on-Data-size-larger-than-Memory-size-td6589.html#a6642)

	In general, one problem with Spark today is that you can OOM under certain configurations, and it's possible you'll need to change from the default configuration if you're using doing very memory-intensive jobs. However, there are very few cases where Spark would simply fail as a matter of course -- for instance, you can always increase the number of partitions to decrease the size of any given one. or repartition data to eliminate skew.
	


## Spark (D)
1. [com.google.protobuf out of memory](http://apache-spark-user-list.1001560.n3.nabble.com/com-google-protobuf-out-of-memory-td6357.html#a6373) (D)
	
	I am getting a OutOfMemoryError in class ByteString.java from package com.google.protobuf when processing very large data using spark 0.9. 

2. [Kafka streaming out of memory](http://apache-spark-user-list.1001560.n3.nabble.com/Kafka-streaming-out-of-memory-td2639.html) (D)

	 When I use the kafka streaming with spark, the rdds store in memory and never be realesed, so OOM will happen after time. 	

3. [Maximum memory limits](http://apache-spark-user-list.1001560.n3.nabble.com/Maximum-memory-limits-td2717.html) (D)

	I was thinking that jblas is going to call native malloc right? 
4. [Pyspark Memory Woes](http://apache-spark-user-list.1001560.n3.nabble.com/Pyspark-Memory-Woes-td2538.html#a4093) (D)

	We're seeing various OOM problems: sometimes python takes all available mem, sometimes we OOM with no heap space left, and occasionally OOM with GC overhead limit exceeded.  I realise it's hard to troubleshoot in the absence of code but any test case we have would be contrived. It seems we should be setting it to (SPARK_WORKER_MEMORY + pyspark memory) / # of concurrent applications, but is there any advice on how to balance memory between executors and pyspark, or does it depend too much on the workload? How do we know if we're getting the most bang for our buck, so to speak?
5. [Spark 0.9.1 java.lang.outOfMemoryError: Java Heap Space](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-0-9-1-java-lang-outOfMemoryError-Java-Heap-Space-td7861.html)

	I am trying to process a file that contains 4 log lines (not very long) and then write my parsed out case classes to a destination folder, and I get the following error:
	


## MLlib (E)
1.  [Running out of memory Naive Bayes](http://apache-spark-user-list.1001560.n3.nabble.com/Running-out-of-memory-Naive-Bayes-td4866.html#a4876) (E)
	
	Each example in the dataset is about 2 million features, only about 20-50 of which are non-zero, so the vectors are very sparse. I keep running out of memory though, even for about 1000 examples on 30gb RAM while the entire dataset is 4 million examples.
	
## Spark Streaming
1. [Out of memory when spark streaming](http://apache-spark-user-list.1001560.n3.nabble.com/help-me-Out-of-memory-when-spark-streaming-td5854.html)

	I send data to spark streaming through Zeromq at a speed of 600 records per second, but the spark streaming only handle 10 records per 5 seconds( set it in streaming program)
	
## Memory-related issues
1. [Spark Memory Bounds](http://apache-spark-user-list.1001560.n3.nabble.com/Spark-Memory-Bounds-td6456.html#a6500) (VD)

	Spark's memory usage is roughly the following:

	(A) In-Memory RDD use + (B) In memory Shuffle use + (C) Transient memory used by all currently running tasks.

2. [Pyspark Memory Woes](http://apache-spark-user-list.1001560.n3.nabble.com/Pyspark-Memory-Woes-td2538.html)

	We're seeing various OOM problems: sometimes python takes all available mem, sometimes we OOM with no heap space left, and occasionally OOM with GC overhead limit exceeded.
3. [ExternalAppendOnlyMap: Spilling in-memory map](http://apache-spark-user-list.1001560.n3.nabble.com/ExternalAppendOnlyMap-Spilling-in-memory-map-td6186.html#a6221)

	 The ExternalAppendOnlyMap is used when a shuffle is causing too much data to be held in memory.  Rather than OOM'ing, Spark writes the data out to disk in a sorted order and reads it back from disk later on when it's needed.  That's the job of the ExternalAppendOnlyMap.
4. [Local Standalone Application and shuffle spills](http://apache-spark-user-list.1001560.n3.nabble.com/Local-Standalone-Application-and-shuffle-spills-td2634.html#a2680)

	"spark.shuffle.spill" refers to a different behavior -- if the "reduce" phase of your shuffle would otherwise cause Spark to OOM, it will instead write data to temporary files on disk. You probably don't want to disable this unless you'd prefer to tune Spark to make sure the reduce can stay in memory.
5. [Performance and serialization: use case](http://apache-spark-user-list.1001560.n3.nabble.com/Performance-and-serialization-use-case-td1513.html#a1615)
	- SERIALIZATION: 
		- Is it OK to manipulate RDDs of Record objects or should we stick with simple RDDs of strings, and do all the splitting and computation in each transformation ? 
		- How to make that efficient in terms of memory and speed? 
		- I've read the docs about the Tuning (and Kryo serialization) but I'd like to have more info on that... 
	- PERFORMANCE: 
		- Is it a good idea to perform all the filters first, and then the groupBy customer, or should we do the reverse? 
		- In the second situation, how can we filter on the values? I didn't see a filterValues method in the PairRDD API ? 


## Interesting questions

1. Is there a way to know which key,value pair is resulting in the OOM?
2. Is there a way to set parallelism in the map stage so that each worker will process one key at time?
3. I was wondering how Spark handles congestion when the upstream is generating dstreams faster than downstream workers can handle? It will eventually OOM.
4. Is there a reason that this value couldn't be dynamically adjusted in response to actual heap usage?
5. Is there a way to automatically re-spawn spark workers? We've situations where executor OOM causes worker process to be DEAD and it does not came back automatically. 
6. Why am I seeing OOMs to begin with? 
7.  How can I reduce the likelihood of seeing OOMs 
8.  Why does an OOM seem to break the executor so hopelessly?
9.  By default 66% of the executor memory is used for RDD caching, so if there's no explicit caching in the code (eg. rdd.cache(), rdd.persiste(StorageLevel.MEM_AND_DISK) etc), this ram is wasted?
10. Is it possible to get the memory usage of one single task in JVM with GC running in the background? 

	you could run 1-core slaves. That way they would only work on one task at a time.
	
	The way I understand it, Spark does not have a tight control on the memory. Your code running on the executor can easily use more than 40% of memory. Spark only limits the memory used for RDD caches and shuffles. If its RDD caches are full, taking up 60% of the heap, and your code takes up more than 40% (after GC), the executor will die with OOM.
	
	heap dump => thread analysis 
	
	Try [heapaudit](https://github.com/foursquare/heapaudit)

11. I want to know that in the case when the size of HBase Table grows larger than the size of RAM available in the cluster, will the application fail, or will there be an impact in performance ?

## Solutions:
1. If the foldByKey solution doesn't work for you, my team uses RDD.persist(DISK_ONLY) to avoid OOM errors.
2. Is your RDD of Strings?  If so, you should make sure to use the Kryo serializer instead of the default Java one.  It stores strings as UTF8 rather than Java's default UTF16 representation, which can save you half the memory usage in the right situation.
3. If an individual partition becomes too large to fit in memory then the usual approach would be to repartition to more partitions, so each one is smaller. Hopefully then it would fit.


## Knowledge
1. When you launch each job, you set SPARK_MEM in its environment to control executors' memory usage. So for example SPARK_MEM=10g java -jar MyCode.jar. SPARK_MEM will automatically be propagated to the slaves. Although you can also set SPARK_MEM in conf/spark-env.sh, it's recommended not to do this, because that would set the same value for every job; the ability to set it there is just left-over from previous versions of Spark and we eventually want to remove it. (I realize it's confusing though.) 

2. SPARK_WORKER_MEMORY is only there to tell each worker daemon the *total* amount of memory on the machine, so that it can know how many concurrent jobs it can safely run (based on each job's SPARK_MEM). In 99% of cases, you will not have to set SPARK_WORKER_MEMORY; the Worker process automatically detects the amount of memory available. 

3. [KafkaInputDStream mapping of partitions to tasks](http://apache-spark-user-list.1001560.n3.nabble.com/KafkaInputDStream-mapping-of-partitions-to-tasks-td3360.html#a3378)
4.  In general, one problem with Spark today is that you can OOM under certain configurations, and it's possible you'll need to change from the default configuration if you're using doing very memory-intensive jobs. However, there are very few cases where Spark would simply fail as a matter of course -- for instance, you can always increase the number of partitions to decrease the size of any given one. or repartition data to eliminate skew.
5.  groupByKey is an expensive transformation, and collecting all the data to driver side may simply cause OOM if the data canâ€™t fit in the driver node.
6.  If your job is using external sorting to avoid OOMing (which it will warn you about in the executor logs with messages like "Spilling in-memory map..."), then you may have arbitrarily many files open. This is very unlikely to happen if you've split your input into as many files as you said, though.

## Ideas
1. display and control the memory usage of code.
2. off-heap approach
3. weak references
